{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import os \n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TAGS\n",
    "UNPROCESSED = 'UNPROCESSED'\n",
    "PROCESSED = 'PROCESSED'\n",
    "QUEUE = 'QUEUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToGraph(line):\n",
    "    fields = line.split()\n",
    "    id = int(fields[0])\n",
    "    connections = []\n",
    "    for connection in fields[1:]:\n",
    "        connections.append(int(connection))\n",
    "\n",
    "    tag = UNPROCESSED\n",
    "    distance = 9999\n",
    "\n",
    "    if (id == startId):\n",
    "        tag = QUEUE\n",
    "        distance = 0\n",
    "\n",
    "    return (id, (connections, distance, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandGraph(node):\n",
    "    id = node[0]\n",
    "    data = node[1]\n",
    "    connections = data[0]\n",
    "    distance = data[1]\n",
    "    tag = data[2]\n",
    "    results = []\n",
    "    \n",
    "    if (tag == QUEUE):\n",
    "        for connection in connections:\n",
    "            newId = connection\n",
    "            newDistance = distance + 1\n",
    "            newTag = QUEUE\n",
    "            if (targetId == connection):\n",
    "                hitCounter.add(1)\n",
    "\n",
    "            newEntry = (newId, ([], newDistance, newTag))\n",
    "            results.append(newEntry)\n",
    "        \n",
    "        tag = PROCESSED\n",
    "    \n",
    "    results.append( (id, (connections, distance, tag)) )\n",
    "    return results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceGraph(data1, data2):\n",
    "    node1 = data1[0]\n",
    "    node2 = data2[0]\n",
    "    distance1 = data1[1]\n",
    "    distance2 = data2[1]\n",
    "    tag1 = data1[2]\n",
    "    tag2 = data2[2]\n",
    "\n",
    "    distance = 9999\n",
    "    tag = tag1\n",
    "    nodes = []\n",
    "\n",
    "    # See if one is the original node with its connections.\n",
    "    # If so preserve them.\n",
    "    if (len(node1) > 0):\n",
    "        nodes.extend(node1)\n",
    "    if (len(node2) > 0):\n",
    "        nodes.extend(node2)\n",
    "\n",
    "    # Preserve minimum distance\n",
    "    if (distance1 < distance):\n",
    "        distance = distance1\n",
    "\n",
    "    if (distance2 < distance):\n",
    "        distance = distance2\n",
    "\n",
    "    # Move queues along\n",
    "    if (tag1 == UNPROCESSED and (tag2 == QUEUE or tag2 == PROCESSED)):\n",
    "        tag = tag2\n",
    "\n",
    "    if (tag1 == QUEUE and tag2 == PROCESSED):\n",
    "        tag = tag2\n",
    "\n",
    "    if (tag2 == UNPROCESSED and (tag1 == QUEUE or tag1 == PROCESSED)):\n",
    "        tag = tag1\n",
    "\n",
    "    if (tag2 == QUEUE and tag1 == PROCESSED):\n",
    "        tag = tag1\n",
    "\n",
    "    return (nodes, distance, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Set up\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"BreadthFirstSearch\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search IDs. Change start and target IDs to get differnt search results\n",
    "startId = 5306\n",
    "targetId = 100\n",
    "hitCounter = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BFS iteration# 1\n",
      "Processing 8330 values.\n",
      "Running BFS iteration# 2\n",
      "Processing 220615 values.\n",
      "Hit the target id! From 4 different direction(s).\n"
     ]
    }
   ],
   "source": [
    "#Main code\n",
    "inputFile = sc.textFile(\"test-data/data.txt\")\n",
    "BFSrdd = inputFile.map(convertToGraph)\n",
    "\n",
    "for iter in range(0, 10):\n",
    "    print(\"Running BFS iteration# \" + str(iter+1))\n",
    "    \n",
    "    # Expand the graph to explore unprocessed nodes\n",
    "    mapped = BFSrdd.flatMap(expandGraph)\n",
    "\n",
    "    print(\"Processing \" + str(mapped.count()) + \" values.\")\n",
    "\n",
    "    if (hitCounter.value > 0):\n",
    "        print(\"Hit the target id! From \" + str(hitCounter.value) \\\n",
    "            + \" different direction(s).\")\n",
    "        break\n",
    "\n",
    "    # Reducer combines data for each character ID\n",
    "    BFSrdd = mapped.reduceByKey(reduceGraph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
